{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n"
     ]
    }
   ],
   "source": [
    "print('Day 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning with [this](https://medium.com/octavian-ai/how-to-get-started-with-machine-learning-on-graphs-7f0795c83763) on how to get started with ML on graphs.\n",
    "\n",
    "Some insights important for us:\n",
    "\n",
    "> In this article I’m not going to cover “traditional” graph analysis — that’s the well known algorithmic techniques like PageRank, clique identification, shortest path, etc. These are very powerful and should be considered a first port-of-call due to their well understood nature and plentiful implementation in public libraries.\n",
    "\n",
    "Dark Web data analysis problems do not conform to these formalisms due to their breadth.\n",
    "\n",
    "> In mainstream areas of ML the community has discovered widely applicable techniques (e.g. transfer learning using ResNet for images or BERT for text) and made them accessible to developers (e.g. TensorFlow, PyTorch, FastAI). However, there are no equivalently easy, universal techniques, nor do any of the popular ML libraries have support for graph data.\n",
    "\n",
    "Challenges faced while applying ML on graphs:\n",
    "\n",
    "- flexibility of graphs doesn’t fit the fixed computation-graph\n",
    "- so we do not get fixed sized tensors, a paradigm popular with deep learning libraries and GPU manufacturers\n",
    "- hence, it is customary (as of Q1, 2019) to build our own high level systems\n",
    "\n",
    "While sampling from a graph to feed to an ML model, two cases might arise:\n",
    "\n",
    "- for small graphs, we can use the adjacency matrix etc to get a tensor of minibatches\n",
    "- for large graphs, we will need a method to sample subgraphs from the graph and train on them.  An extreme case is to take individual node-edge pairs.\n",
    "\n",
    "> Note that some approaches tabularize the data before it reaches the machine learning library. Node2Vec is a good example of this, where random walks are used to transform each node into a vector. These vectors are then fed into the machine learning model as a list.\n",
    "\n",
    "Common types of graph ML problems:\n",
    "\n",
    "- scoring of nodes\n",
    "- scoring of edges\n",
    "- scoring of graphs\n",
    "- link prediction\n",
    "\n",
    "A very common way to vectorize graphs is through embeddings, like through using random walks. We should definitely try Node2Vec on our problem (a team member will be working on this)\n",
    "\n",
    "Graph Convolutional Networks, or simply, Graph Networks are a very broad area of research on graph ML, and I'll be focusing in on them from the next episode.\n",
    "\n",
    "Finally, recent approaches use techniques like attention and reinforcement learning to solve graph ML problems, and this might be the next thing to try after GCNs.\n",
    "\n",
    "TBC with these:\n",
    "- https://arxiv.org/abs/1806.01261\n",
    "- https://tkipf.github.io/graph-convolutional-networks/\n",
    "- https://arxiv.org/pdf/1812.08434v1.pdf\n",
    "- https://nlp.stanford.edu/pubs/SocherChenManningNg_NIPS2013.pdf\n",
    "\n",
    "And look at latest work on GCNs.\n",
    "\n",
    "Bye!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 2\n"
     ]
    }
   ],
   "source": [
    "print('Day 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving on to read up papers, let's look at the current research landscape on graph learning and DL on graphs, as of Jul, 2020.\n",
    "\n",
    "Edit: Much of these results should be very insightful for us once we're clear how vanilla GCNs work.\n",
    "\n",
    "- Around 10% of all accepted papers ar tier-1 conferences are on Graph Networks\n",
    "- The width of the network, w * the depth of the network, d should be proportional to the size of the graph n, i.e. dw = O(n), if we want GNN being able to compute a solution to popular graph problems (like cycle detection, diameter estimation, vertex cover, etc.). This is currently not the case.\n",
    "- This year there are applications in fixing bugs in Javascript, game playing, answering IQ-like tests, optimization of TensorFlow computational graphs, molecule generation, and question generation in dialogue systems.\n",
    "- There are quite a few papers this year on reasoning in knowledge graphs. Essentially, a knowledge graph is a structured way to represent facts, where nodes and edges carry meaningful information.\n",
    "\n",
    "[This](https://arxiv.org/pdf/1912.12693.pdf) review paper is a very recent (Jun, 2020) study on the theory of various graph DL approaches.\n",
    "\n",
    "[This](https://towardsdatascience.com/graph-deep-learning/home) is an ongoing blog series on various topics on graph NNs, might be a useful read in the future.\n",
    "\n",
    "From now on, I'll combine it under the umbrella term \"Geometric Deep Learning\".\n",
    "\n",
    "> A major difference compared to classical deep neural networks dealing with grid-structured data is that on graphs such operations are permutation-invariant, i.e. independent of the order of neighbour nodes, as there is usually no canonical way of ordering them.\n",
    "\n",
    "As of 2020, we have two professionally maintained libraries that can be used to build GDL models, namely [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) and [DGL](https://www.dgl.ai/). \n",
    "\n",
    "[This](http://geometricdeeplearning.com/) website has a collection of tutorials, papers and code on GDL.\n",
    "\n",
    "Some nice latest talks on GDL:\n",
    "- [Microsoft](https://www.youtube.com/watch?v=zCEYiCxrL_0)\n",
    "- [Prof. Xavier Bresson](https://www.youtube.com/watch?v=VXNjCAmb6Zw)\n",
    "- [Institute for Advanced Study](https://www.youtube.com/watch?v=USfNJNePDKQ)\n",
    "- [Stanford](https://www.youtube.com/watch?v=YrhBZUtgG4E)\n",
    "\n",
    "So, the path ahead is clear: watch these lectures, read basic papers, get the theory clear, look up code implementations, and finally switch to our dataset. A number of recent works on GDL involve other fields like NLP, CV, RL, Encoders, GANs etc, and we might need to study them up later.\n",
    "\n",
    "Starting with [this](https://www.youtube.com/watch?v=zCEYiCxrL_0) video lecture. Summary: \n",
    "- Distributed Embeddings\n",
    "- Graph repn of problem -> repn of each node using Dist. Embd.\n",
    "- Neural Message Passing\n",
    "- Invariance through unification operation\n",
    "- Two types - Gated GNNs and GCNs\n",
    "- Applications - molecular chemistry, finding bugs in code\n",
    "- Special cases - CNNs, Deep Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3\n"
     ]
    }
   ],
   "source": [
    "print('Day 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://arxiv.org/abs/1609.02907) is the seminal paper on GCNs by Kipf and Welling (ICLR 2017)\n",
    "\n",
    "`../papers/GCN-Kipf-2017.pdf` has reading highlights, notes, etc. After a reading, we can move on to look at implementations in PyTorch. Notes:\n",
    "- Spectral Graph Convs and Chebyshev Polynomials on Laplacians, etc; most math isn't clear rn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
