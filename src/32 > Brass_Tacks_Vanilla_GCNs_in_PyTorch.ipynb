{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "32 > Brass Tacks Vanilla GCNs in PyTorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU9D21DXQmsg",
        "colab_type": "text"
      },
      "source": [
        "The nuts and bolts of Deep Graph Learning, with loading graph data and training a Vanilla GCN in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDXNmqzP7jkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf0cb2f8-9fbb-41b4-a16a-e9cb3c41a8f4"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Cv_pAy8x_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://github.com/tkipf/pygcn/raw/master/data/cora/cora.cites\n",
        "! wget -q https://github.com/tkipf/pygcn/raw/master/data/cora/cora.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB0Ogz8UEF70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qXsZbm40boo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_onehot(labels):\n",
        "  classes      = set(labels)\n",
        "  classes_dict = {c : np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "  labels_oh    = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "  return labels_oh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0y3irJ2ziT",
        "colab_type": "text"
      },
      "source": [
        "## The Dataset (Cora)\n",
        "\n",
        "The Cora dataset consists of ML papers, classified into one of the following seven classes:\n",
        "-\tCase_Based\n",
        "-\tGenetic_Algorithms\n",
        "-\tNeural_Networks\n",
        "-\tProbabilistic_Methods\n",
        "-\tReinforcement_Learning\n",
        "-\tRule_Learning\n",
        "-\tTheory\n",
        "\n",
        "The papers were selected in a way such that in the final corpus every paper cites or is cited by atleast one other paper. There are 2708 papers in the whole corpus. \n",
        "\n",
        "After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. \n",
        "\n",
        "`cora.content` contains descriptions of the papers in the following format:\n",
        "\n",
        "\t\t<paper_id> <word_attributes> <class_label>\n",
        "\n",
        "The first entry in each line contains the unique string ID of the paper followed by binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper. Finally, the last entry in the line contains the class label of the paper.\n",
        "\n",
        "`cora.cites` contains the citation graph of the corpus. Each line describes a link in the following format:\n",
        "\n",
        "\t\t<ID of cited paper> <ID of citing paper>\n",
        "\n",
        "Each line contains two paper IDs. The first entry is the ID of the paper being cited and the second ID stands for the paper which contains the citation. The direction of the link is from right to left. If a line is represented by \"paper1 paper2\" then the link is \"paper2->paper1\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0GPNG8M1QU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path='./', dataset='cora'):\n",
        "  idx_features_labels = np.genfromtxt(f'{path}{dataset}.content', dtype=np.dtype(str))\n",
        "  features            = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "  labels              = encode_onehot(idx_features_labels[:, -1])                       # the target\n",
        "  idx                 = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "  idx_map             = {j : i for i, j in enumerate(idx)}                              # build graph\n",
        "  edges_unordered     = np.genfromtxt(f'{path}{dataset}.cites', dtype=np.int32)\n",
        "  edges               = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32)\n",
        "  edges               = edges.reshape(edges_unordered.shape)\n",
        "  edges_t             = (np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1]))\n",
        "  adj                 = sp.coo_matrix(edges_t, shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "  adj                 = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)   # symmetric \n",
        "  features            = normalize(features)\n",
        "  adj                 = normalize(adj + sp.eye(adj.shape[0]))\n",
        "  idx_train           = range(140)\n",
        "  idx_val             = range(200, 500)\n",
        "  idx_test            = range(500, 1500)\n",
        "  features            = torch.FloatTensor(np.array(features.todense()))\n",
        "  labels              = torch.LongTensor(np.where(labels)[1])\n",
        "  adj                 = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "  idx_train           = torch.LongTensor(idx_train)\n",
        "  idx_val             = torch.LongTensor(idx_val)\n",
        "  idx_test            = torch.LongTensor(idx_test)\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWwtAx_n1QZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(mx):\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  r_inv  = np.power(rowsum, -1).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.\n",
        "  r_mat_inv = sp.diags(r_inv)\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds   = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2WImyf1QeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODik85BG4QYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fc61ae8-ca04-45a8-e869-411d635c0869"
      },
      "source": [
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2708, 1433])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZnqKRbN4Qdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "687b9a63-8a8c-469e-a9ee-8d1fc2611b37"
      },
      "source": [
        "adj.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2708, 2708])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsd3gMPw4QgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e42fc55-7ccb-4b90-9fab-feefef2932b7"
      },
      "source": [
        "min([sum(features[z]) for z in range(2708)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twlbLJEi4QbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "c747c728-7dd1-4902-97ef-d401f8e69e18"
      },
      "source": [
        "idx_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
              "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
              "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
              "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
              "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
              "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
              "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
              "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
              "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
              "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1_GUNxp9F8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCLk5B7DDdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphConv(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super(GraphConv, self).__init__()\n",
        "    self.in_features  = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "    if bias:\n",
        "      self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "    else:\n",
        "      self.register_parameter('bias', None)\n",
        "    self.reset_parameters()\n",
        "    \n",
        "  def reset_parameters(self):\n",
        "    stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "    self.weight.data.uniform_(-stdv, stdv)\n",
        "    if self.bias is not None:\n",
        "      self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input, adj):\n",
        "    support = torch.mm(input, self.weight)\n",
        "    output = torch.spmm(adj, support)\n",
        "    if self.bias is not None:\n",
        "        return output + self.bias\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "      return self.__class__.__name__ +' ('+str(self.in_features)+' -> '+str(self.out_features)+')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-i7AIU4D-Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VanillaGCN(nn.Module):\n",
        "\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "    super(VanillaGCN, self).__init__()\n",
        "    self.gc1 = GraphConv(nfeat, nhid)\n",
        "    self.gc2 = GraphConv(nhid, nclass)\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    x = F.relu(self.gc1(x, adj))\n",
        "    x = F.dropout(x, self.dropout, training=self.training)\n",
        "    x = self.gc2(x, adj)\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah8cnedjDV5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if CUDA: torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYTovIOKTsHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.01\n",
        "epochs = 200\n",
        "wd = 5e-4\n",
        "hidden = 16\n",
        "dropout = 0.5\n",
        "fastmode = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_VgUNPOT_ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VanillaGCN(nfeat=features.shape[1], nhid=hidden, nclass=labels.max().item() + 1, dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRBuPNDMD5Ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CUDA:\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  adj = adj.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "  idx_val = idx_val.cuda()\n",
        "  idx_test = idx_test.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lRmy_Y6SVqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  t = time.time()\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "  loss_train.backward()\n",
        "  optimizer.step()\n",
        "  if not fastmode:\n",
        "      model.eval()\n",
        "      output = model(features, adj)\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "  return loss_train.item(), loss_val.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qEw8X6gSlet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  output = model(features, adj)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.item()),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BjAiO1tSOSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0fff001-342e-4f2b-faac-3314f6610959"
      },
      "source": [
        "import time\n",
        "t_total = time.time()\n",
        "train_losses, val_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "  loss_train, loss_val = train(epoch)\n",
        "  train_losses.append(loss_train)\n",
        "  val_losses.append(loss_val)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 1.9756 acc_train: 0.1071 loss_val: 1.9451 acc_val: 0.1033 time: 0.0269s\n",
            "Epoch: 0002 loss_train: 1.9480 acc_train: 0.1071 loss_val: 1.9284 acc_val: 0.1033 time: 0.0199s\n",
            "Epoch: 0003 loss_train: 1.9359 acc_train: 0.1214 loss_val: 1.9128 acc_val: 0.1067 time: 0.0175s\n",
            "Epoch: 0004 loss_train: 1.9229 acc_train: 0.2000 loss_val: 1.8981 acc_val: 0.3767 time: 0.0166s\n",
            "Epoch: 0005 loss_train: 1.9007 acc_train: 0.2714 loss_val: 1.8841 acc_val: 0.3500 time: 0.0170s\n",
            "Epoch: 0006 loss_train: 1.8943 acc_train: 0.2929 loss_val: 1.8706 acc_val: 0.3500 time: 0.0194s\n",
            "Epoch: 0007 loss_train: 1.8763 acc_train: 0.2929 loss_val: 1.8575 acc_val: 0.3500 time: 0.0169s\n",
            "Epoch: 0008 loss_train: 1.8596 acc_train: 0.2929 loss_val: 1.8445 acc_val: 0.3500 time: 0.0170s\n",
            "Epoch: 0009 loss_train: 1.8480 acc_train: 0.2929 loss_val: 1.8318 acc_val: 0.3500 time: 0.0177s\n",
            "Epoch: 0010 loss_train: 1.8312 acc_train: 0.2929 loss_val: 1.8196 acc_val: 0.3500 time: 0.0154s\n",
            "Epoch: 0011 loss_train: 1.8224 acc_train: 0.2929 loss_val: 1.8080 acc_val: 0.3500 time: 0.0173s\n",
            "Epoch: 0012 loss_train: 1.8021 acc_train: 0.2929 loss_val: 1.7970 acc_val: 0.3500 time: 0.0213s\n",
            "Epoch: 0013 loss_train: 1.7923 acc_train: 0.2929 loss_val: 1.7866 acc_val: 0.3500 time: 0.0180s\n",
            "Epoch: 0014 loss_train: 1.7894 acc_train: 0.2929 loss_val: 1.7768 acc_val: 0.3500 time: 0.0177s\n",
            "Epoch: 0015 loss_train: 1.7768 acc_train: 0.2929 loss_val: 1.7674 acc_val: 0.3500 time: 0.0180s\n",
            "Epoch: 0016 loss_train: 1.7657 acc_train: 0.2929 loss_val: 1.7584 acc_val: 0.3500 time: 0.0183s\n",
            "Epoch: 0017 loss_train: 1.7423 acc_train: 0.2929 loss_val: 1.7499 acc_val: 0.3500 time: 0.0158s\n",
            "Epoch: 0018 loss_train: 1.7396 acc_train: 0.2929 loss_val: 1.7418 acc_val: 0.3500 time: 0.0167s\n",
            "Epoch: 0019 loss_train: 1.7355 acc_train: 0.2929 loss_val: 1.7342 acc_val: 0.3500 time: 0.0167s\n",
            "Epoch: 0020 loss_train: 1.7195 acc_train: 0.3000 loss_val: 1.7272 acc_val: 0.3500 time: 0.0154s\n",
            "Epoch: 0021 loss_train: 1.7110 acc_train: 0.2929 loss_val: 1.7205 acc_val: 0.3500 time: 0.0154s\n",
            "Epoch: 0022 loss_train: 1.6935 acc_train: 0.3143 loss_val: 1.7139 acc_val: 0.3500 time: 0.0169s\n",
            "Epoch: 0023 loss_train: 1.6951 acc_train: 0.3286 loss_val: 1.7074 acc_val: 0.3500 time: 0.0174s\n",
            "Epoch: 0024 loss_train: 1.6794 acc_train: 0.3214 loss_val: 1.7007 acc_val: 0.3500 time: 0.0192s\n",
            "Epoch: 0025 loss_train: 1.6766 acc_train: 0.3071 loss_val: 1.6937 acc_val: 0.3500 time: 0.0184s\n",
            "Epoch: 0026 loss_train: 1.6741 acc_train: 0.3357 loss_val: 1.6864 acc_val: 0.3533 time: 0.0162s\n",
            "Epoch: 0027 loss_train: 1.6394 acc_train: 0.3357 loss_val: 1.6787 acc_val: 0.3600 time: 0.0150s\n",
            "Epoch: 0028 loss_train: 1.6374 acc_train: 0.3357 loss_val: 1.6705 acc_val: 0.3633 time: 0.0160s\n",
            "Epoch: 0029 loss_train: 1.6236 acc_train: 0.3500 loss_val: 1.6619 acc_val: 0.3633 time: 0.0158s\n",
            "Epoch: 0030 loss_train: 1.6135 acc_train: 0.3786 loss_val: 1.6530 acc_val: 0.3600 time: 0.0142s\n",
            "Epoch: 0031 loss_train: 1.5814 acc_train: 0.3857 loss_val: 1.6438 acc_val: 0.3667 time: 0.0148s\n",
            "Epoch: 0032 loss_train: 1.5897 acc_train: 0.4071 loss_val: 1.6344 acc_val: 0.3700 time: 0.0166s\n",
            "Epoch: 0033 loss_train: 1.5612 acc_train: 0.4214 loss_val: 1.6250 acc_val: 0.3767 time: 0.0178s\n",
            "Epoch: 0034 loss_train: 1.5524 acc_train: 0.4143 loss_val: 1.6155 acc_val: 0.3933 time: 0.0168s\n",
            "Epoch: 0035 loss_train: 1.5465 acc_train: 0.4357 loss_val: 1.6061 acc_val: 0.4067 time: 0.0168s\n",
            "Epoch: 0036 loss_train: 1.5103 acc_train: 0.4857 loss_val: 1.5966 acc_val: 0.4300 time: 0.0184s\n",
            "Epoch: 0037 loss_train: 1.5202 acc_train: 0.4857 loss_val: 1.5865 acc_val: 0.4500 time: 0.0153s\n",
            "Epoch: 0038 loss_train: 1.4913 acc_train: 0.5357 loss_val: 1.5759 acc_val: 0.4633 time: 0.0137s\n",
            "Epoch: 0039 loss_train: 1.4959 acc_train: 0.5214 loss_val: 1.5646 acc_val: 0.4733 time: 0.0143s\n",
            "Epoch: 0040 loss_train: 1.4417 acc_train: 0.5429 loss_val: 1.5528 acc_val: 0.4900 time: 0.0145s\n",
            "Epoch: 0041 loss_train: 1.4493 acc_train: 0.5143 loss_val: 1.5403 acc_val: 0.4933 time: 0.0152s\n",
            "Epoch: 0042 loss_train: 1.4453 acc_train: 0.5429 loss_val: 1.5276 acc_val: 0.4967 time: 0.0140s\n",
            "Epoch: 0043 loss_train: 1.4443 acc_train: 0.5071 loss_val: 1.5147 acc_val: 0.5000 time: 0.0152s\n",
            "Epoch: 0044 loss_train: 1.3950 acc_train: 0.5357 loss_val: 1.5021 acc_val: 0.5167 time: 0.0144s\n",
            "Epoch: 0045 loss_train: 1.3669 acc_train: 0.5929 loss_val: 1.4893 acc_val: 0.5267 time: 0.0154s\n",
            "Epoch: 0046 loss_train: 1.3917 acc_train: 0.5429 loss_val: 1.4768 acc_val: 0.5333 time: 0.0145s\n",
            "Epoch: 0047 loss_train: 1.3555 acc_train: 0.5571 loss_val: 1.4646 acc_val: 0.5333 time: 0.0158s\n",
            "Epoch: 0048 loss_train: 1.3355 acc_train: 0.5857 loss_val: 1.4524 acc_val: 0.5400 time: 0.0168s\n",
            "Epoch: 0049 loss_train: 1.3194 acc_train: 0.6143 loss_val: 1.4402 acc_val: 0.5400 time: 0.0179s\n",
            "Epoch: 0050 loss_train: 1.3184 acc_train: 0.5857 loss_val: 1.4281 acc_val: 0.5533 time: 0.0157s\n",
            "Epoch: 0051 loss_train: 1.2862 acc_train: 0.5857 loss_val: 1.4158 acc_val: 0.5533 time: 0.0158s\n",
            "Epoch: 0052 loss_train: 1.2816 acc_train: 0.6143 loss_val: 1.4038 acc_val: 0.5667 time: 0.0144s\n",
            "Epoch: 0053 loss_train: 1.2731 acc_train: 0.6143 loss_val: 1.3919 acc_val: 0.5833 time: 0.0154s\n",
            "Epoch: 0054 loss_train: 1.2331 acc_train: 0.6571 loss_val: 1.3803 acc_val: 0.6000 time: 0.0153s\n",
            "Epoch: 0055 loss_train: 1.2189 acc_train: 0.6500 loss_val: 1.3691 acc_val: 0.6200 time: 0.0147s\n",
            "Epoch: 0056 loss_train: 1.2022 acc_train: 0.6857 loss_val: 1.3581 acc_val: 0.6233 time: 0.0145s\n",
            "Epoch: 0057 loss_train: 1.1786 acc_train: 0.6714 loss_val: 1.3470 acc_val: 0.6300 time: 0.0140s\n",
            "Epoch: 0058 loss_train: 1.1720 acc_train: 0.6786 loss_val: 1.3357 acc_val: 0.6267 time: 0.0146s\n",
            "Epoch: 0059 loss_train: 1.1649 acc_train: 0.7214 loss_val: 1.3237 acc_val: 0.6300 time: 0.0157s\n",
            "Epoch: 0060 loss_train: 1.1458 acc_train: 0.6643 loss_val: 1.3109 acc_val: 0.6433 time: 0.0151s\n",
            "Epoch: 0061 loss_train: 1.1706 acc_train: 0.6857 loss_val: 1.2979 acc_val: 0.6600 time: 0.0144s\n",
            "Epoch: 0062 loss_train: 1.1263 acc_train: 0.6929 loss_val: 1.2851 acc_val: 0.6633 time: 0.0167s\n",
            "Epoch: 0063 loss_train: 1.1350 acc_train: 0.7214 loss_val: 1.2723 acc_val: 0.6700 time: 0.0160s\n",
            "Epoch: 0064 loss_train: 1.0997 acc_train: 0.7143 loss_val: 1.2599 acc_val: 0.6767 time: 0.0156s\n",
            "Epoch: 0065 loss_train: 1.0853 acc_train: 0.7286 loss_val: 1.2476 acc_val: 0.6800 time: 0.0205s\n",
            "Epoch: 0066 loss_train: 1.0461 acc_train: 0.7500 loss_val: 1.2357 acc_val: 0.6867 time: 0.0164s\n",
            "Epoch: 0067 loss_train: 1.0417 acc_train: 0.7714 loss_val: 1.2239 acc_val: 0.6900 time: 0.0162s\n",
            "Epoch: 0068 loss_train: 1.0237 acc_train: 0.7357 loss_val: 1.2125 acc_val: 0.6933 time: 0.0152s\n",
            "Epoch: 0069 loss_train: 1.0438 acc_train: 0.7571 loss_val: 1.2012 acc_val: 0.7033 time: 0.0150s\n",
            "Epoch: 0070 loss_train: 1.0503 acc_train: 0.7357 loss_val: 1.1903 acc_val: 0.7067 time: 0.0148s\n",
            "Epoch: 0071 loss_train: 1.0236 acc_train: 0.7643 loss_val: 1.1796 acc_val: 0.7033 time: 0.0144s\n",
            "Epoch: 0072 loss_train: 0.9898 acc_train: 0.7571 loss_val: 1.1692 acc_val: 0.7067 time: 0.0148s\n",
            "Epoch: 0073 loss_train: 0.9770 acc_train: 0.7714 loss_val: 1.1590 acc_val: 0.7100 time: 0.0146s\n",
            "Epoch: 0074 loss_train: 0.9595 acc_train: 0.7571 loss_val: 1.1489 acc_val: 0.7133 time: 0.0162s\n",
            "Epoch: 0075 loss_train: 0.9530 acc_train: 0.7929 loss_val: 1.1384 acc_val: 0.7167 time: 0.0172s\n",
            "Epoch: 0076 loss_train: 0.9343 acc_train: 0.8071 loss_val: 1.1279 acc_val: 0.7200 time: 0.0151s\n",
            "Epoch: 0077 loss_train: 0.9276 acc_train: 0.7857 loss_val: 1.1177 acc_val: 0.7300 time: 0.0150s\n",
            "Epoch: 0078 loss_train: 0.9309 acc_train: 0.8071 loss_val: 1.1078 acc_val: 0.7300 time: 0.0160s\n",
            "Epoch: 0079 loss_train: 0.9054 acc_train: 0.8071 loss_val: 1.0980 acc_val: 0.7333 time: 0.0231s\n",
            "Epoch: 0080 loss_train: 0.8663 acc_train: 0.8071 loss_val: 1.0885 acc_val: 0.7367 time: 0.0187s\n",
            "Epoch: 0081 loss_train: 0.8975 acc_train: 0.7857 loss_val: 1.0792 acc_val: 0.7467 time: 0.0147s\n",
            "Epoch: 0082 loss_train: 0.9040 acc_train: 0.8000 loss_val: 1.0701 acc_val: 0.7467 time: 0.0149s\n",
            "Epoch: 0083 loss_train: 0.8721 acc_train: 0.8214 loss_val: 1.0615 acc_val: 0.7500 time: 0.0158s\n",
            "Epoch: 0084 loss_train: 0.8526 acc_train: 0.8286 loss_val: 1.0529 acc_val: 0.7567 time: 0.0157s\n",
            "Epoch: 0085 loss_train: 0.8459 acc_train: 0.8143 loss_val: 1.0442 acc_val: 0.7600 time: 0.0151s\n",
            "Epoch: 0086 loss_train: 0.8053 acc_train: 0.8143 loss_val: 1.0357 acc_val: 0.7633 time: 0.0156s\n",
            "Epoch: 0087 loss_train: 0.7878 acc_train: 0.8500 loss_val: 1.0271 acc_val: 0.7700 time: 0.0180s\n",
            "Epoch: 0088 loss_train: 0.7960 acc_train: 0.8500 loss_val: 1.0183 acc_val: 0.7733 time: 0.0146s\n",
            "Epoch: 0089 loss_train: 0.7804 acc_train: 0.8286 loss_val: 1.0100 acc_val: 0.7767 time: 0.0150s\n",
            "Epoch: 0090 loss_train: 0.8083 acc_train: 0.8214 loss_val: 1.0016 acc_val: 0.7767 time: 0.0144s\n",
            "Epoch: 0091 loss_train: 0.7987 acc_train: 0.8357 loss_val: 0.9932 acc_val: 0.7800 time: 0.0147s\n",
            "Epoch: 0092 loss_train: 0.8256 acc_train: 0.8357 loss_val: 0.9855 acc_val: 0.7800 time: 0.0142s\n",
            "Epoch: 0093 loss_train: 0.7929 acc_train: 0.8429 loss_val: 0.9786 acc_val: 0.7833 time: 0.0162s\n",
            "Epoch: 0094 loss_train: 0.7798 acc_train: 0.8500 loss_val: 0.9720 acc_val: 0.7800 time: 0.0150s\n",
            "Epoch: 0095 loss_train: 0.7348 acc_train: 0.8500 loss_val: 0.9654 acc_val: 0.7800 time: 0.0182s\n",
            "Epoch: 0096 loss_train: 0.7236 acc_train: 0.8357 loss_val: 0.9582 acc_val: 0.7833 time: 0.0148s\n",
            "Epoch: 0097 loss_train: 0.7086 acc_train: 0.8643 loss_val: 0.9508 acc_val: 0.7833 time: 0.0158s\n",
            "Epoch: 0098 loss_train: 0.7240 acc_train: 0.8786 loss_val: 0.9427 acc_val: 0.7967 time: 0.0149s\n",
            "Epoch: 0099 loss_train: 0.7753 acc_train: 0.8500 loss_val: 0.9351 acc_val: 0.7933 time: 0.0190s\n",
            "Epoch: 0100 loss_train: 0.7145 acc_train: 0.8571 loss_val: 0.9281 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0101 loss_train: 0.7089 acc_train: 0.8500 loss_val: 0.9218 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0102 loss_train: 0.6844 acc_train: 0.8929 loss_val: 0.9157 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0103 loss_train: 0.7044 acc_train: 0.8571 loss_val: 0.9096 acc_val: 0.8000 time: 0.0156s\n",
            "Epoch: 0104 loss_train: 0.7052 acc_train: 0.8643 loss_val: 0.9033 acc_val: 0.7967 time: 0.0139s\n",
            "Epoch: 0105 loss_train: 0.6305 acc_train: 0.8786 loss_val: 0.8967 acc_val: 0.7967 time: 0.0156s\n",
            "Epoch: 0106 loss_train: 0.6765 acc_train: 0.8857 loss_val: 0.8908 acc_val: 0.8033 time: 0.0140s\n",
            "Epoch: 0107 loss_train: 0.6312 acc_train: 0.9071 loss_val: 0.8857 acc_val: 0.8000 time: 0.0138s\n",
            "Epoch: 0108 loss_train: 0.6488 acc_train: 0.8857 loss_val: 0.8813 acc_val: 0.8033 time: 0.0143s\n",
            "Epoch: 0109 loss_train: 0.6602 acc_train: 0.8643 loss_val: 0.8766 acc_val: 0.7967 time: 0.0154s\n",
            "Epoch: 0110 loss_train: 0.6423 acc_train: 0.8786 loss_val: 0.8715 acc_val: 0.8000 time: 0.0139s\n",
            "Epoch: 0111 loss_train: 0.6210 acc_train: 0.8929 loss_val: 0.8657 acc_val: 0.7900 time: 0.0132s\n",
            "Epoch: 0112 loss_train: 0.6237 acc_train: 0.8643 loss_val: 0.8601 acc_val: 0.7900 time: 0.0162s\n",
            "Epoch: 0113 loss_train: 0.6451 acc_train: 0.8786 loss_val: 0.8553 acc_val: 0.8000 time: 0.0146s\n",
            "Epoch: 0114 loss_train: 0.6194 acc_train: 0.8714 loss_val: 0.8507 acc_val: 0.8033 time: 0.0148s\n",
            "Epoch: 0115 loss_train: 0.5926 acc_train: 0.8929 loss_val: 0.8465 acc_val: 0.8067 time: 0.0153s\n",
            "Epoch: 0116 loss_train: 0.5899 acc_train: 0.8929 loss_val: 0.8428 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0117 loss_train: 0.5705 acc_train: 0.8929 loss_val: 0.8388 acc_val: 0.8133 time: 0.0144s\n",
            "Epoch: 0118 loss_train: 0.6579 acc_train: 0.8571 loss_val: 0.8350 acc_val: 0.8133 time: 0.0151s\n",
            "Epoch: 0119 loss_train: 0.6208 acc_train: 0.8643 loss_val: 0.8314 acc_val: 0.8100 time: 0.0143s\n",
            "Epoch: 0120 loss_train: 0.5951 acc_train: 0.8857 loss_val: 0.8290 acc_val: 0.8133 time: 0.0187s\n",
            "Epoch: 0121 loss_train: 0.5646 acc_train: 0.8643 loss_val: 0.8272 acc_val: 0.8133 time: 0.0163s\n",
            "Epoch: 0122 loss_train: 0.5566 acc_train: 0.8929 loss_val: 0.8255 acc_val: 0.8000 time: 0.0162s\n",
            "Epoch: 0123 loss_train: 0.5833 acc_train: 0.8857 loss_val: 0.8224 acc_val: 0.8000 time: 0.0157s\n",
            "Epoch: 0124 loss_train: 0.5529 acc_train: 0.8929 loss_val: 0.8181 acc_val: 0.8033 time: 0.0164s\n",
            "Epoch: 0125 loss_train: 0.5642 acc_train: 0.9143 loss_val: 0.8132 acc_val: 0.8067 time: 0.0206s\n",
            "Epoch: 0126 loss_train: 0.5579 acc_train: 0.9000 loss_val: 0.8094 acc_val: 0.8100 time: 0.0200s\n",
            "Epoch: 0127 loss_train: 0.5864 acc_train: 0.8857 loss_val: 0.8064 acc_val: 0.8100 time: 0.0162s\n",
            "Epoch: 0128 loss_train: 0.5227 acc_train: 0.9286 loss_val: 0.8033 acc_val: 0.8133 time: 0.0157s\n",
            "Epoch: 0129 loss_train: 0.5209 acc_train: 0.9071 loss_val: 0.8016 acc_val: 0.8067 time: 0.0151s\n",
            "Epoch: 0130 loss_train: 0.5415 acc_train: 0.9071 loss_val: 0.8006 acc_val: 0.7967 time: 0.0146s\n",
            "Epoch: 0131 loss_train: 0.5455 acc_train: 0.9214 loss_val: 0.7989 acc_val: 0.8000 time: 0.0147s\n",
            "Epoch: 0132 loss_train: 0.5499 acc_train: 0.8857 loss_val: 0.7960 acc_val: 0.8000 time: 0.0141s\n",
            "Epoch: 0133 loss_train: 0.5285 acc_train: 0.9143 loss_val: 0.7921 acc_val: 0.8033 time: 0.0146s\n",
            "Epoch: 0134 loss_train: 0.5315 acc_train: 0.9143 loss_val: 0.7876 acc_val: 0.8033 time: 0.0145s\n",
            "Epoch: 0135 loss_train: 0.5395 acc_train: 0.9214 loss_val: 0.7832 acc_val: 0.8133 time: 0.0162s\n",
            "Epoch: 0136 loss_train: 0.5243 acc_train: 0.9143 loss_val: 0.7795 acc_val: 0.8200 time: 0.0160s\n",
            "Epoch: 0137 loss_train: 0.5214 acc_train: 0.9000 loss_val: 0.7767 acc_val: 0.8133 time: 0.0189s\n",
            "Epoch: 0138 loss_train: 0.5611 acc_train: 0.9071 loss_val: 0.7745 acc_val: 0.8100 time: 0.0148s\n",
            "Epoch: 0139 loss_train: 0.4819 acc_train: 0.9429 loss_val: 0.7734 acc_val: 0.8000 time: 0.0164s\n",
            "Epoch: 0140 loss_train: 0.5186 acc_train: 0.9143 loss_val: 0.7729 acc_val: 0.8000 time: 0.0159s\n",
            "Epoch: 0141 loss_train: 0.4968 acc_train: 0.8929 loss_val: 0.7720 acc_val: 0.8000 time: 0.0171s\n",
            "Epoch: 0142 loss_train: 0.4923 acc_train: 0.9214 loss_val: 0.7706 acc_val: 0.7967 time: 0.0152s\n",
            "Epoch: 0143 loss_train: 0.4834 acc_train: 0.9357 loss_val: 0.7687 acc_val: 0.7933 time: 0.0195s\n",
            "Epoch: 0144 loss_train: 0.4973 acc_train: 0.9286 loss_val: 0.7651 acc_val: 0.7967 time: 0.0156s\n",
            "Epoch: 0145 loss_train: 0.5217 acc_train: 0.9000 loss_val: 0.7607 acc_val: 0.7967 time: 0.0148s\n",
            "Epoch: 0146 loss_train: 0.4637 acc_train: 0.9286 loss_val: 0.7563 acc_val: 0.8067 time: 0.0145s\n",
            "Epoch: 0147 loss_train: 0.4776 acc_train: 0.9071 loss_val: 0.7531 acc_val: 0.8167 time: 0.0150s\n",
            "Epoch: 0148 loss_train: 0.4639 acc_train: 0.8857 loss_val: 0.7504 acc_val: 0.8167 time: 0.0146s\n",
            "Epoch: 0149 loss_train: 0.5256 acc_train: 0.8857 loss_val: 0.7474 acc_val: 0.8233 time: 0.0152s\n",
            "Epoch: 0150 loss_train: 0.4697 acc_train: 0.9286 loss_val: 0.7449 acc_val: 0.8233 time: 0.0187s\n",
            "Epoch: 0151 loss_train: 0.5265 acc_train: 0.9143 loss_val: 0.7431 acc_val: 0.8233 time: 0.0142s\n",
            "Epoch: 0152 loss_train: 0.4466 acc_train: 0.9286 loss_val: 0.7416 acc_val: 0.8233 time: 0.0140s\n",
            "Epoch: 0153 loss_train: 0.4808 acc_train: 0.9286 loss_val: 0.7407 acc_val: 0.8167 time: 0.0149s\n",
            "Epoch: 0154 loss_train: 0.5003 acc_train: 0.9071 loss_val: 0.7406 acc_val: 0.8067 time: 0.0151s\n",
            "Epoch: 0155 loss_train: 0.4704 acc_train: 0.9214 loss_val: 0.7412 acc_val: 0.8000 time: 0.0152s\n",
            "Epoch: 0156 loss_train: 0.4875 acc_train: 0.9286 loss_val: 0.7418 acc_val: 0.8033 time: 0.0154s\n",
            "Epoch: 0157 loss_train: 0.4975 acc_train: 0.9071 loss_val: 0.7409 acc_val: 0.8033 time: 0.0148s\n",
            "Epoch: 0158 loss_train: 0.4448 acc_train: 0.9286 loss_val: 0.7398 acc_val: 0.8033 time: 0.0148s\n",
            "Epoch: 0159 loss_train: 0.4222 acc_train: 0.9286 loss_val: 0.7379 acc_val: 0.8000 time: 0.0143s\n",
            "Epoch: 0160 loss_train: 0.4539 acc_train: 0.9286 loss_val: 0.7350 acc_val: 0.8033 time: 0.0154s\n",
            "Epoch: 0161 loss_train: 0.4650 acc_train: 0.9286 loss_val: 0.7318 acc_val: 0.8067 time: 0.0215s\n",
            "Epoch: 0162 loss_train: 0.4263 acc_train: 0.9214 loss_val: 0.7288 acc_val: 0.8133 time: 0.0242s\n",
            "Epoch: 0163 loss_train: 0.4272 acc_train: 0.9571 loss_val: 0.7264 acc_val: 0.8167 time: 0.0174s\n",
            "Epoch: 0164 loss_train: 0.4682 acc_train: 0.9214 loss_val: 0.7246 acc_val: 0.8167 time: 0.0161s\n",
            "Epoch: 0165 loss_train: 0.5016 acc_train: 0.9000 loss_val: 0.7227 acc_val: 0.8167 time: 0.0142s\n",
            "Epoch: 0166 loss_train: 0.4303 acc_train: 0.9357 loss_val: 0.7213 acc_val: 0.8100 time: 0.0147s\n",
            "Epoch: 0167 loss_train: 0.4148 acc_train: 0.9143 loss_val: 0.7202 acc_val: 0.8100 time: 0.0140s\n",
            "Epoch: 0168 loss_train: 0.4779 acc_train: 0.8857 loss_val: 0.7187 acc_val: 0.8100 time: 0.0157s\n",
            "Epoch: 0169 loss_train: 0.4283 acc_train: 0.9357 loss_val: 0.7176 acc_val: 0.8033 time: 0.0145s\n",
            "Epoch: 0170 loss_train: 0.4433 acc_train: 0.9429 loss_val: 0.7173 acc_val: 0.7933 time: 0.0155s\n",
            "Epoch: 0171 loss_train: 0.4344 acc_train: 0.9429 loss_val: 0.7171 acc_val: 0.7933 time: 0.0155s\n",
            "Epoch: 0172 loss_train: 0.4654 acc_train: 0.9286 loss_val: 0.7155 acc_val: 0.7933 time: 0.0153s\n",
            "Epoch: 0173 loss_train: 0.4524 acc_train: 0.9214 loss_val: 0.7136 acc_val: 0.7967 time: 0.0162s\n",
            "Epoch: 0174 loss_train: 0.4689 acc_train: 0.8929 loss_val: 0.7114 acc_val: 0.8067 time: 0.0170s\n",
            "Epoch: 0175 loss_train: 0.4167 acc_train: 0.8929 loss_val: 0.7095 acc_val: 0.8133 time: 0.0182s\n",
            "Epoch: 0176 loss_train: 0.3964 acc_train: 0.9357 loss_val: 0.7088 acc_val: 0.8167 time: 0.0153s\n",
            "Epoch: 0177 loss_train: 0.4173 acc_train: 0.9571 loss_val: 0.7073 acc_val: 0.8167 time: 0.0142s\n",
            "Epoch: 0178 loss_train: 0.3868 acc_train: 0.9429 loss_val: 0.7064 acc_val: 0.8133 time: 0.0148s\n",
            "Epoch: 0179 loss_train: 0.3891 acc_train: 0.9429 loss_val: 0.7063 acc_val: 0.8133 time: 0.0154s\n",
            "Epoch: 0180 loss_train: 0.4258 acc_train: 0.9500 loss_val: 0.7061 acc_val: 0.8167 time: 0.0154s\n",
            "Epoch: 0181 loss_train: 0.4083 acc_train: 0.9286 loss_val: 0.7067 acc_val: 0.8167 time: 0.0147s\n",
            "Epoch: 0182 loss_train: 0.4125 acc_train: 0.9357 loss_val: 0.7082 acc_val: 0.8133 time: 0.0154s\n",
            "Epoch: 0183 loss_train: 0.4387 acc_train: 0.9357 loss_val: 0.7077 acc_val: 0.8100 time: 0.0159s\n",
            "Epoch: 0184 loss_train: 0.3856 acc_train: 0.9643 loss_val: 0.7058 acc_val: 0.8100 time: 0.0152s\n",
            "Epoch: 0185 loss_train: 0.4281 acc_train: 0.9643 loss_val: 0.7021 acc_val: 0.8100 time: 0.0259s\n",
            "Epoch: 0186 loss_train: 0.4136 acc_train: 0.9214 loss_val: 0.6987 acc_val: 0.8133 time: 0.0183s\n",
            "Epoch: 0187 loss_train: 0.3936 acc_train: 0.9429 loss_val: 0.6951 acc_val: 0.8167 time: 0.0174s\n",
            "Epoch: 0188 loss_train: 0.4321 acc_train: 0.9214 loss_val: 0.6925 acc_val: 0.8133 time: 0.0151s\n",
            "Epoch: 0189 loss_train: 0.4113 acc_train: 0.9214 loss_val: 0.6908 acc_val: 0.8100 time: 0.0148s\n",
            "Epoch: 0190 loss_train: 0.4379 acc_train: 0.9357 loss_val: 0.6900 acc_val: 0.8133 time: 0.0153s\n",
            "Epoch: 0191 loss_train: 0.3700 acc_train: 0.9643 loss_val: 0.6904 acc_val: 0.8167 time: 0.0154s\n",
            "Epoch: 0192 loss_train: 0.3824 acc_train: 0.9429 loss_val: 0.6915 acc_val: 0.8233 time: 0.0149s\n",
            "Epoch: 0193 loss_train: 0.3731 acc_train: 0.9429 loss_val: 0.6929 acc_val: 0.8233 time: 0.0138s\n",
            "Epoch: 0194 loss_train: 0.3736 acc_train: 0.9500 loss_val: 0.6936 acc_val: 0.8133 time: 0.0152s\n",
            "Epoch: 0195 loss_train: 0.3757 acc_train: 0.9571 loss_val: 0.6939 acc_val: 0.8167 time: 0.0157s\n",
            "Epoch: 0196 loss_train: 0.3637 acc_train: 0.9500 loss_val: 0.6921 acc_val: 0.8167 time: 0.0184s\n",
            "Epoch: 0197 loss_train: 0.3836 acc_train: 0.9429 loss_val: 0.6907 acc_val: 0.8200 time: 0.0145s\n",
            "Epoch: 0198 loss_train: 0.3888 acc_train: 0.9643 loss_val: 0.6888 acc_val: 0.8133 time: 0.0159s\n",
            "Epoch: 0199 loss_train: 0.3399 acc_train: 0.9429 loss_val: 0.6882 acc_val: 0.8100 time: 0.0152s\n",
            "Epoch: 0200 loss_train: 0.3710 acc_train: 0.9500 loss_val: 0.6872 acc_val: 0.8100 time: 0.0178s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4128s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aOHpfjmTyQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15fbd951-c705-44ff-ac48-50341ced71e5"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set results: loss= 0.7075 accuracy= 0.8350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgTfVX7CUySC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLmJbYibYj_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "1125e9fe-d2b0-4519-e90b-ede0b928705c"
      },
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f71cf82e9e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gilds_d0Ve82",
        "colab_type": "text"
      },
      "source": [
        "Sometime soon, we'll have week-13 of [this](https://www.youtube.com/watch?v=f01J0Dri-6k&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=24&t=0s) and that should be really good!\n",
        "\n",
        "Also, ICML 2020 had a workshop on [GRL+](https://slideslive.com/icml-2020/graph-representation-learning-and-beyond-grl), should definitely watch relevant presentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYum6i8VeRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}